{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result of this exercise should be a file named `prepare.py` that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define a function named `basic_clean`. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "    - Lowercase everything\n",
    "    - Normalize unicode characters\n",
    "    - Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and applies basic text cleaning by:\n",
    "    lowercasing everything,\n",
    "    normalizing unicode characters,\n",
    "    replacing anything that is not a letter, number, whitespace, or a single quote\n",
    "    \n",
    "    and returns the cleaned string.\n",
    "    '''\n",
    "    \n",
    "    #lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    #normalize unicode chars\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    #replace anything not a letter, number, whitespace, or single quote\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello i am testing this function i wonder if this'll work\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function\n",
    "string = \"Hello, I am testing this function! I wonder if this'll work?\"\n",
    "string = basic_clean(string)\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a function named `tokenize`. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string, \n",
    "    tokenizes all the words in the string,\n",
    "    \n",
    "    and returns the tokenized string.\n",
    "    '''\n",
    "    \n",
    "    #create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello i am testing this function i wonder if this ' ll work\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function\n",
    "string = tokenize(string)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a function named `stem`. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in some text\n",
    "    \n",
    "    and returns the text after applying stemming to all the words.\n",
    "    '''\n",
    "    \n",
    "    #create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    #apply the stemmer to each word in the string\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    #join stemmed list of words into a string again\n",
    "    string_stemmed = ' '.join(stems)\n",
    "    \n",
    "    return string_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello i am test thi function i wonder if thi ' ll work\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function\n",
    "string = stem(string)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define a function named `lemmatize`. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in some text\n",
    "    \n",
    "    and returns the text after applying lemmatization to each word.\n",
    "    '''\n",
    "    \n",
    "    #create the lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    #use the lemmatizer on each word in the list of words created by using split\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    #join lemmatized list of words into a string again\n",
    "    string_lemmatized = ' '.join(lemmas)\n",
    "    \n",
    "    return string_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello i am testing this function i wonder if this ' ll work\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function w/ lemmatize instead of stem\n",
    "string = \"Hello, I am testing this function! I wonder if this'll work?\"\n",
    "string = basic_clean(string)\n",
    "string = tokenize(string)\n",
    "string = lemmatize(string)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define a function named `remove_stopwords`. It should accept some text and return the text after removing all the stopwords. \n",
    "\n",
    "- This function should define two optional parameters, `extra_words` and `exclude_words`. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []): \n",
    "    '''\n",
    "    This function takes in a string, \n",
    "    optional extra_words (additional stop words),\n",
    "    and optional exclude_words (words that won't be removed) parameters with default empty lists,\n",
    "    \n",
    "    and returns a string after removing all the stopwords.\n",
    "    '''\n",
    "\n",
    "    #create stopword_list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    #remove 'exclude_words' from stopword_list to keep these in my text\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    #add in 'extra_words' to stopword_list\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    #split words in string\n",
    "    words = string.split()\n",
    "    \n",
    "    #create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    #join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello testing function wonder ' work\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function\n",
    "string = remove_stopwords(string)\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i testing function i wonder if ' work\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function w/ extra words and exlude words specified\n",
    "string = \"Hello, I am testing this function! I wonder if this'll work?\"\n",
    "string = basic_clean(string)\n",
    "string = tokenize(string)\n",
    "string = lemmatize(string)\n",
    "\n",
    "string = remove_stopwords(string, extra_words = ['hello'], exclude_words = ['if', 'i'])\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe `news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['business', 'sports', 'technology', 'entertainment']\n",
    "news_df = pd.DataFrame(acquire.get_news_articles(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Air India pilots demand vaccination on priorit...</td>\n",
       "      <td>Indian Commercial Pilots Association (ICPA) on...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India underestimated the coronavirus: Raghuram...</td>\n",
       "      <td>Speaking about India's second COVID-19 wave, f...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea's richest woman gets fortune worth...</td>\n",
       "      <td>South Korea’s richest woman Hong Ra-hee added ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World's biggest jeweller says it will no longe...</td>\n",
       "      <td>Pandora, the world's biggest jeweller, has sai...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will supply 11 cr doses to states, pvt hospita...</td>\n",
       "      <td>Serum Institute of India (SII) CEO Adar Poonaw...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Everything's a mess, have stopped using Instag...</td>\n",
       "      <td>Calling the ongoing coronavirus pandemic in In...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>A film I'm super proud of: Hansal on 3 years o...</td>\n",
       "      <td>Filmmaker Hansal Mehta took to Instagram on Tu...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Hope Bell Bottom releases in theatres: Huma on...</td>\n",
       "      <td>Amid reports of director Ranjit Tewari's 'Bell...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>World is interested in negative: Britney on do...</td>\n",
       "      <td>Taking to Instagram, singer Britney Spears sha...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Mohanlal's 'Drishyam 2' to get a Hindi remake</td>\n",
       "      <td>Kumar Mangat Pathak and Abhishek Pathak's Pano...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Air India pilots demand vaccination on priorit...   \n",
       "1   India underestimated the coronavirus: Raghuram...   \n",
       "2   South Korea's richest woman gets fortune worth...   \n",
       "3   World's biggest jeweller says it will no longe...   \n",
       "4   Will supply 11 cr doses to states, pvt hospita...   \n",
       "..                                                ...   \n",
       "93  Everything's a mess, have stopped using Instag...   \n",
       "94  A film I'm super proud of: Hansal on 3 years o...   \n",
       "95  Hope Bell Bottom releases in theatres: Huma on...   \n",
       "96  World is interested in negative: Britney on do...   \n",
       "97      Mohanlal's 'Drishyam 2' to get a Hindi remake   \n",
       "\n",
       "                                              content       category  \n",
       "0   Indian Commercial Pilots Association (ICPA) on...       business  \n",
       "1   Speaking about India's second COVID-19 wave, f...       business  \n",
       "2   South Korea’s richest woman Hong Ra-hee added ...       business  \n",
       "3   Pandora, the world's biggest jeweller, has sai...       business  \n",
       "4   Serum Institute of India (SII) CEO Adar Poonaw...       business  \n",
       "..                                                ...            ...  \n",
       "93  Calling the ongoing coronavirus pandemic in In...  entertainment  \n",
       "94  Filmmaker Hansal Mehta took to Instagram on Tu...  entertainment  \n",
       "95  Amid reports of director Ranjit Tewari's 'Bell...  entertainment  \n",
       "96  Taking to Instagram, singer Britney Spears sha...  entertainment  \n",
       "97  Kumar Mangat Pathak and Abhishek Pathak's Pano...  entertainment  \n",
       "\n",
       "[98 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make another dataframe for the Codeup blog posts. Name the dataframe `codeup_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://codeup.com/codeups-data-science-career-accelerator-is-here/', \n",
    "        'https://codeup.com/data-science-myths/', \n",
    "        'https://codeup.com/data-science-vs-data-analytics-whats-the-difference/', \n",
    "        'https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/', \n",
    "        'https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/']\n",
    "\n",
    "codeup_df = pd.DataFrame(acquire.get_blog_articles(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!   \n",
       "1                                 Data Science Myths   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                             content  \n",
       "0  The rumors are true! The time has arrived. Cod...  \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...  \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...  \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...  \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. For each dataframe, produce the following columns:\n",
    "\n",
    "- `title` to hold the title\n",
    "- `original` to hold the original article/post content\n",
    "- `clean` to hold the normalized and tokenized original with the stopwords removed.\n",
    "- `stemmed` to hold the stemmed version of the cleaned data.\n",
    "- `lemmatized` to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df, \n",
    "    the string name for a text column,\n",
    "    with option to pass lists for extra_words and exclude_words,\n",
    "    renames content column to be 'original',\n",
    "    \n",
    "    and returns a df with the text article title, original text, \n",
    "    cleaned, tokenized, stemmed, and lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    \n",
    "    #rename the content column to original\n",
    "    df = df.rename(columns={\"content\": \"original\"})\n",
    "    \n",
    "    #holds the normalized and tokenized original w/ stopwords removed\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    #holds the stemmed version of the cleaned data\n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(stem)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    #holds the lemmatized version of the cleaned data\n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(lemmatize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    return df[['title', column, 'clean', 'stemmed', 'lemmatized']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Air India pilots demand vaccination on priorit...</td>\n",
       "      <td>Indian Commercial Pilots Association (ICPA) on...</td>\n",
       "      <td>indian commercial pilots association icpa tues...</td>\n",
       "      <td>indian commerci pilot associ icpa tuesday said...</td>\n",
       "      <td>indian commercial pilot association icpa tuesd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India underestimated the coronavirus: Raghuram...</td>\n",
       "      <td>Speaking about India's second COVID-19 wave, f...</td>\n",
       "      <td>speaking india ' second covid19 wave former rb...</td>\n",
       "      <td>speak india ' second covid19 wave former rbi g...</td>\n",
       "      <td>speaking india ' second covid19 wave former rb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea's richest woman gets fortune worth...</td>\n",
       "      <td>South Korea’s richest woman Hong Ra-hee added ...</td>\n",
       "      <td>south koreas richest woman hong rahee added an...</td>\n",
       "      <td>south korea richest woman hong rahe ad anoth 7...</td>\n",
       "      <td>south korea richest woman hong rahee added ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World's biggest jeweller says it will no longe...</td>\n",
       "      <td>Pandora, the world's biggest jeweller, has sai...</td>\n",
       "      <td>pandora world ' biggest jeweller said ' stop u...</td>\n",
       "      <td>pandora world ' biggest jewel said ' stop use ...</td>\n",
       "      <td>pandora world ' biggest jeweller said ' stop u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will supply 11 cr doses to states, pvt hospita...</td>\n",
       "      <td>Serum Institute of India (SII) CEO Adar Poonaw...</td>\n",
       "      <td>serum institute india sii ceo adar poonawalla ...</td>\n",
       "      <td>serum institut india sii ceo adar poonawalla s...</td>\n",
       "      <td>serum institute india sii ceo adar poonawalla ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Air India pilots demand vaccination on priorit...   \n",
       "1  India underestimated the coronavirus: Raghuram...   \n",
       "2  South Korea's richest woman gets fortune worth...   \n",
       "3  World's biggest jeweller says it will no longe...   \n",
       "4  Will supply 11 cr doses to states, pvt hospita...   \n",
       "\n",
       "                                            original  \\\n",
       "0  Indian Commercial Pilots Association (ICPA) on...   \n",
       "1  Speaking about India's second COVID-19 wave, f...   \n",
       "2  South Korea’s richest woman Hong Ra-hee added ...   \n",
       "3  Pandora, the world's biggest jeweller, has sai...   \n",
       "4  Serum Institute of India (SII) CEO Adar Poonaw...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  indian commercial pilots association icpa tues...   \n",
       "1  speaking india ' second covid19 wave former rb...   \n",
       "2  south koreas richest woman hong rahee added an...   \n",
       "3  pandora world ' biggest jeweller said ' stop u...   \n",
       "4  serum institute india sii ceo adar poonawalla ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  indian commerci pilot associ icpa tuesday said...   \n",
       "1  speak india ' second covid19 wave former rbi g...   \n",
       "2  south korea richest woman hong rahe ad anoth 7...   \n",
       "3  pandora world ' biggest jewel said ' stop use ...   \n",
       "4  serum institut india sii ceo adar poonawalla s...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  indian commercial pilot association icpa tuesd...  \n",
       "1  speaking india ' second covid19 wave former rb...  \n",
       "2  south korea richest woman hong rahee added ano...  \n",
       "3  pandora world ' biggest jeweller said ' stop u...  \n",
       "4  serum institute india sii ceo adar poonawalla ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function for news_df original article/post content\n",
    "prep_article_data(news_df, 'original', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "      <td>rumors true time arrived codeup officially ope...</td>\n",
       "      <td>rumor true time arriv codeup offici open appli...</td>\n",
       "      <td>rumor true time arrived codeup officially open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "      <td>dimitri antoniou maggie giust data science big...</td>\n",
       "      <td>dimitri antoni maggi giust data scienc big dat...</td>\n",
       "      <td>dimitri antoniou maggie giust data science big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "      <td>dimitri antoniou week ago codeup launched imme...</td>\n",
       "      <td>dimitri antoni week ago codeup launch immers d...</td>\n",
       "      <td>dimitri antoniou week ago codeup launched imme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "      <td>sa tech job fair third biannual san antonio te...</td>\n",
       "      <td>sa tech job fair third biannual san antonio te...</td>\n",
       "      <td>sa tech job fair third biannual san antonio te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>competitor bootcamps closing model danger prog...</td>\n",
       "      <td>competitor bootcamp close model danger program...</td>\n",
       "      <td>competitor bootcamps closing model danger prog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!   \n",
       "1                                 Data Science Myths   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                            original  \\\n",
       "0  The rumors are true! The time has arrived. Cod...   \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...   \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...   \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  rumors true time arrived codeup officially ope...   \n",
       "1  dimitri antoniou maggie giust data science big...   \n",
       "2  dimitri antoniou week ago codeup launched imme...   \n",
       "3  sa tech job fair third biannual san antonio te...   \n",
       "4  competitor bootcamps closing model danger prog...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  rumor true time arriv codeup offici open appli...   \n",
       "1  dimitri antoni maggi giust data scienc big dat...   \n",
       "2  dimitri antoni week ago codeup launch immers d...   \n",
       "3  sa tech job fair third biannual san antonio te...   \n",
       "4  competitor bootcamp close model danger program...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  rumor true time arrived codeup officially open...  \n",
       "1  dimitri antoniou maggie giust data science big...  \n",
       "2  dimitri antoniou week ago codeup launched imme...  \n",
       "3  sa tech job fair third biannual san antonio te...  \n",
       "4  competitor bootcamps closing model danger prog...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function for codeup_df original article/post content\n",
    "prep_article_data(codeup_df, 'original', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Ask yourself:\n",
    "\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "    - probably lemmatizing due to small text\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "    - it depends how much time you have to waste\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n",
    "    - most probably stemmed to reduce text and it would take too long to lemmatize (plus save money w/ stemmed)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
